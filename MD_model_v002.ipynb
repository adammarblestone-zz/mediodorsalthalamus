{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This has just a PFC and sensory-motor system only, with the PFC as a LSTM. Does not have an MD yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adammarblestone/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters of the network\n",
    "num_units_PFC = 30\n",
    "num_timesteps = 10\n",
    "num_hidden_sensorymotor = 20\n",
    "\n",
    "# Parameters of the training\n",
    "batch_size = 16\n",
    "num_batches = 50\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFINITION OF THE TASK\n",
    "\n",
    "# The \"rule input\" to the sensory motor system, coming from PFC, currently takes the form of just a number, either 1.0 or 2.0\n",
    "# 1.0 indicates to use vision and ignore audition, and 2.0 indicates to use audition and ignore vision\n",
    "def rvis(): \n",
    "    return 1.0 # If the input to the sensory-motor system, x, has 1 <= x < 2 then use vision\n",
    "def raud(): # If the input to the sensory-motor system, x, has x > 2 then use audition\n",
    "    return 2.0\n",
    "def rnull():\n",
    "    return 0.0\n",
    "\n",
    "# This is the function that will be used to evaluate whether the PFC output the correct rule for a given cue, and to train it to do so\n",
    "def cue_to_rule_mapping_function(t, integer_input): # Define a changing mapping from cues to rules\n",
    "    \n",
    "    if t % 10 < 10: # We can use this type of conditioning to set up time-dependent cue-to-rule mappings\n",
    "        r1 = tf.cond(tf.equal(integer_input, tf.constant(0, dtype = tf.float32)), true_fn = rvis, false_fn = rnull)\n",
    "    \n",
    "        r2 = tf.cond(tf.equal(integer_input, tf.constant(1, dtype = tf.float32)), true_fn = rvis, false_fn = rnull)\n",
    "        \n",
    "        r3 = tf.cond(tf.equal(integer_input, tf.constant(2, dtype = tf.float32)), true_fn = raud, false_fn = rnull)\n",
    "        \n",
    "        r4 = tf.cond(tf.equal(integer_input, tf.constant(3, dtype = tf.float32)), true_fn = raud, false_fn = rnull)\n",
    "        \n",
    "    else: # We can use this type of conditioning to set up time-dependent cue-to-rule mappings\n",
    "        r1 = tf.cond(tf.equal(integer_input, tf.constant(0, dtype = tf.float32)), true_fn = raud, false_fn = rnull)\n",
    "    \n",
    "        r2 = tf.cond(tf.equal(integer_input, tf.constant(1, dtype = tf.float32)), true_fn = raud, false_fn = rnull)\n",
    "        \n",
    "        r3 = tf.cond(tf.equal(integer_input, tf.constant(2, dtype = tf.float32)), true_fn = rvis, false_fn = rnull)\n",
    "        \n",
    "        r4 = tf.cond(tf.equal(integer_input, tf.constant(3, dtype = tf.float32)), true_fn = rvis, false_fn = rnull)\n",
    "        \n",
    "    return tf.reduce_max([r1,r2,r3,r4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for evaluating correctness\n",
    "def correctness(q, o, ina, inv, ctin): # Arguments: time, PFC output list, auditory input list, vision input list, cue inputs timeline\n",
    "    c = []\n",
    "    for e in range(batch_size):\n",
    "        if (np.linalg.norm(inv[e,q] - o[q][e]) < np.linalg.norm(ina[e,q] - o[q][e]) and sess.run(cue_to_rule_mapping_function(q, float(ctin[e,q]))) == 1.0):\n",
    "            c.append(\"correct\") # If the output coordinates are closer to the vision target than the auditory target and that's the right rule\n",
    "        elif (np.linalg.norm(inv[e,q] - o[q][e]) > np.linalg.norm(ina[e,q] - o[q][e]) and sess.run(cue_to_rule_mapping_function(q, float(ctin[e,q]))) == 2.0):\n",
    "            c.append(\"correct\") # If the output coordinates are closer to the auditory target than the vision target and that's the right rule\n",
    "        else:\n",
    "            c.append(\"incorrect\")\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 14.577291\n",
      "\n",
      "\n",
      "\n",
      "timestep 0\n",
      "out [[ 0.42918468  0.53500295]\n",
      " [ 0.21194018  0.3292433 ]\n",
      " [ 0.75775373  0.67685711]\n",
      " [ 0.31410295  0.14970599]\n",
      " [ 0.51756424  0.59566307]\n",
      " [ 0.57217234  0.42977095]\n",
      " [ 0.47736132  0.31754237]\n",
      " [ 0.27360153  0.44619381]\n",
      " [ 0.6442256   0.27659923]\n",
      " [ 0.46477169  0.20845914]\n",
      " [ 0.43677932  0.55619371]\n",
      " [ 0.65188408  0.28989965]\n",
      " [ 0.34884784  0.41045201]\n",
      " [ 0.42507499  0.25425535]\n",
      " [ 0.35185787  0.67104661]\n",
      " [ 0.4523102   0.39098591]] \n",
      "vis in [[ 0.18770674  0.62347284]\n",
      " [ 0.09035809  0.59689095]\n",
      " [ 0.87773248  0.90608053]\n",
      " [ 0.03184005  0.15371884]\n",
      " [ 0.98509245  0.91739489]\n",
      " [ 0.11301817  0.00146032]\n",
      " [ 0.3008774   0.51705328]\n",
      " [ 0.34985754  0.50114751]\n",
      " [ 0.26548769  0.09189342]\n",
      " [ 0.35486895  0.31953991]\n",
      " [ 0.83963938  0.9085419 ]\n",
      " [ 0.47931122  0.10887283]\n",
      " [ 0.23307975  0.17399051]\n",
      " [ 0.27448972  0.03350521]\n",
      " [ 0.33559309  0.75860506]\n",
      " [ 0.50019015  0.26431855]] \n",
      "aud in [[ 0.69671029  0.51985281]\n",
      " [ 0.23854383  0.13636152]\n",
      " [ 0.89042021  0.5513405 ]\n",
      " [ 0.39205478  0.08389936]\n",
      " [ 0.13447857  0.20337039]\n",
      " [ 0.71072253  0.81909521]\n",
      " [ 0.62717527  0.10724905]\n",
      " [ 0.06683083  0.53376332]\n",
      " [ 0.93766843  0.21469712]\n",
      " [ 0.42183601  0.17832991]\n",
      " [ 0.08352713  0.16109505]\n",
      " [ 0.65550345  0.58732249]\n",
      " [ 0.18596515  0.78264354]\n",
      " [ 0.27035088  0.59746862]\n",
      " [ 0.41739582  0.69879673]\n",
      " [ 0.23295139  0.67513643]] \n",
      "rule chosen [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]] \n",
      "correct rule"
     ]
    }
   ],
   "source": [
    "# Define the network\n",
    "\n",
    "DO_SHARE = None # This is needed for the recurrency of the RNN, see: https://blog.evjang.com/2016/06/understanding-and-implementing.html\n",
    "\n",
    "PFC_state_previous = tf.contrib.rnn.LSTMBlockCell(num_units = num_units_PFC).zero_state(batch_size, tf.float32) # Initial state of PFC\n",
    "\n",
    "# This does one cycle of the PFC RNN\n",
    "def PFC_step(input_data, network_state):\n",
    "    with tf.variable_scope(\"PFC\", reuse=DO_SHARE):\n",
    "        PFC_cell = tf.contrib.rnn.LSTMBlockCell(num_units = num_units_PFC)\n",
    "        return PFC_cell(inputs = input_data, state = network_state) \n",
    "    \n",
    "# Cue inputs delivered INTO the PFC, e.g., high pass noise, UV light, and so forth, here defined currently as just an integer indiciating which cue\n",
    "cue_timeseries = tf.placeholder(shape=[batch_size, num_timesteps, 1], dtype=tf.float32, name = 'cues_timeseries')\n",
    "\n",
    "# Visual and audio targets input to the sensory-motor system, represented as just 2D coordinates, one for each, right now\n",
    "input_vis = tf.placeholder(shape=[batch_size, num_timesteps, 2], dtype=tf.float32, name = 'input_vis')\n",
    "input_aud = tf.placeholder(shape=[batch_size, num_timesteps, 2], dtype=tf.float32, name = 'input_aud')\n",
    "\n",
    "loss = 0 # This will accumulate contributions to the loss function\n",
    "outs = [] # Output coordinates from the sensory-motor system\n",
    "rules_chosen = [] # Rules input from the PFC to the sensory-motor system\n",
    "\n",
    "for t in range(num_timesteps): # Iterating over time in the experiment, with batches implicitly being used at each time step\n",
    "    \n",
    "    current_cue = cue_timeseries[:,t] # This is the cue that gets fed into the PFC on this timestep\n",
    "\n",
    "    PFC_state = PFC_step(input_data = current_cue, network_state = PFC_state_previous)\n",
    "    # The output from the PFC into the sensorymotor system gives information about the rule that the sensory-motor system is supposed to execute\n",
    "    with tf.variable_scope(\"PFC_output\", reuse=DO_SHARE):\n",
    "        PFC_output = tf.contrib.layers.fully_connected(PFC_state[0], 1, activation_fn = tf.nn.relu) # Note the 1-dimensional rule output\n",
    "    rules_chosen.append(PFC_output) # The info sent by PFC to the sensory-motor system, corresponding to the chosen rule\n",
    "\n",
    "    input_rule_to_sensorymotor_system = PFC_output # The rule output from the PFC is sent to the sensory-motor system\n",
    "    input_total = tf.concat([input_vis[:,t], input_aud[:,t], input_rule_to_sensorymotor_system], axis = -1) # Sensory-motor system gets aud, vis & rule inputs\n",
    "    \n",
    "    with tf.variable_scope(\"hidden_sensorymotor\", reuse=DO_SHARE):\n",
    "        hidden_sensorymotor = tf.contrib.layers.fully_connected(input_total, num_hidden_sensorymotor, activation_fn = tf.nn.relu) # Sensory-motor hidden layer\n",
    "    \n",
    "    num_out_sensorymotor = 2 # These are the 2D coordinates that we want to sensory-motor system to produce\n",
    "    \n",
    "    with tf.variable_scope(\"out_sensorymotor\", reuse=DO_SHARE):\n",
    "        out_sensorymotor = tf.contrib.layers.fully_connected(hidden_sensorymotor, num_out_sensorymotor, activation_fn = tf.nn.relu) \n",
    "        # Output coord from sensory-motor system\n",
    "    \n",
    "    outs.append(out_sensorymotor)\n",
    "    \n",
    "    PFC_state_previous = PFC_state[1] # This is needed for the recurrency of the PFC RNN\n",
    "    \n",
    "    for e in range(batch_size): # Here is where we compute a contribution to the loss function for training\n",
    "        cc = current_cue[e][0] # The current cue at this time and this position in the batch\n",
    "        cue_to_rule_mapping_function_output = cue_to_rule_mapping_function(t, cc) # The CORRECT rule to choose in the task\n",
    "        if t>=0: # Always true here but we'll make this change later -- we can use this to set up time-dependent loss functions, e.g., only look at late times\n",
    "            loss_contrib1 = tf.cond(tf.equal(cue_to_rule_mapping_function_output, tf.constant(1, dtype = tf.float32)),\\\n",
    "                                    true_fn = lambda: tf.square(tf.norm(input_vis[e,t] - out_sensorymotor[e])),  false_fn = lambda: 0.0)\n",
    "            loss_contrib2 = tf.cond(tf.equal(cue_to_rule_mapping_function_output, tf.constant(2, dtype = tf.float32)),\\\n",
    "                                    true_fn = lambda: tf.square(tf.norm(input_aud[e,t] - out_sensorymotor[e])),  false_fn = lambda: 0.0)\n",
    "            # The last two lines define the loss function contribution at a given time, \n",
    "            # here the proximity of the output coordinate to the visual input target OR the proximity to the auditory target,\n",
    "            # depending on the correct rule for this moment in the task\n",
    "            loss += (loss_contrib1 + loss_contrib2)\n",
    "            \n",
    "    DO_SHARE = True # This is needed for the recurrency of the RNN, see: https://blog.evjang.com/2016/06/understanding-and-implementing.html\n",
    "\n",
    "# Setting up the session\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op=optimizer.minimize(loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "# This is the cue timeseries input provided by the environment\n",
    "# Here random cues are delivered across time and within batch\n",
    "ct = np.reshape([np.random.randint(0,4) for k in range(num_timesteps * batch_size * num_batches)], [num_batches, batch_size, num_timesteps, 1])\n",
    "    \n",
    "# Randomly chosen visual target coordinates\n",
    "in_vis_list_unshaped = [[np.random.rand(), np.random.rand()] for k in range(num_timesteps * batch_size * num_batches)] \n",
    "in_vis_list = np.reshape(in_vis_list_unshaped, [num_batches,batch_size,num_timesteps, 2])\n",
    "\n",
    "# Randomly chosen auditory target coordinates\n",
    "in_aud_list_unshaped = [[np.random.rand(), np.random.rand()] for k in range(num_timesteps * batch_size * num_batches)]\n",
    "in_aud_list = np.reshape(in_aud_list_unshaped, [num_batches,batch_size,num_timesteps, 2])\n",
    "\n",
    "# Running the session and storing the loss function across batches to see the learning curve  \n",
    "losses = []\n",
    "for b in range(num_batches): # Running the training batches\n",
    "    ct_in = ct[b, :, :]\n",
    "    in_v = in_vis_list[b, :, :]\n",
    "    in_a = in_aud_list[b, :, :]\n",
    "    os, l, rc, _ = sess.run([outs, loss, rules_chosen, train_op], feed_dict = {cue_timeseries:ct_in, input_vis: in_v, input_aud:in_a})\n",
    "    # The outputs here are just for the last batch only...\n",
    "    losses.append(l)\n",
    "                \n",
    "# Printing the outputs for visual inspection\n",
    "print \"Loss: %f\" % l\n",
    "print \"\\n\\n\"\n",
    "for q in range(num_timesteps):\n",
    "    print \"timestep %i\" % q\n",
    "    print \"out\", os[q][:], \"\\n\", \"vis in\", in_v[:,q],\"\\n\", \"aud in\", in_a[:, q],\"\\n\",\\\n",
    "    \"rule chosen\", rc[q][:],\"\\n\", \"correct rule\",\\\n",
    "    [sess.run(cue_to_rule_mapping_function(q, float(ct_in[e,q]))) for e in range(batch_size)]  \n",
    "    print \"correctness:\", correctness(q, os, in_a, in_v, ct_in) \n",
    "    print \"\\n\"\n",
    "\n",
    "# Plotting the loss function over time to see the learning process\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.title(\"Loss versus # batches\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
