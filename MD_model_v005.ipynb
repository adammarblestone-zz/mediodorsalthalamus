{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making PFC not trainable, then adding an MD in this version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adammarblestone/Library/Enthought/Canopy_64bit/User/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters of the network\n",
    "num_units_PFC = 70\n",
    "num_hidden_sensorymotor = 30\n",
    "PFC_output_dimensions = 25\n",
    "resting_prefactor = 0.03 # This scales the relative contribution of the waiting / inactive state to the loss function\n",
    "PFC_trainable = False\n",
    "\n",
    "\n",
    "# Parameters of the training\n",
    "batch_size = 16\n",
    "num_batches = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Parameters of the delay task: we can later make these TF placeholders to train over multiple such\n",
    "num_timesteps = 20\n",
    "start_time_of_delay = 6\n",
    "end_time_of_delay = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFINITION OF THE TASK\n",
    "\n",
    "# The \"rule input\" to the sensory motor system, coming from PFC, currently takes the form of just a number, either 1.0 or 2.0\n",
    "# 1.0 indicates to use vision and ignore audition, and 2.0 indicates to use audition and ignore vision\n",
    "def rvis(): \n",
    "    return 1.0 # If the input to the sensory-motor system, x, has 1 <= x < 2 then use vision\n",
    "def raud(): # If the input to the sensory-motor system, x, has x > 2 then use audition\n",
    "    return 2.0\n",
    "def rnull(): # We'll use rnull output to indicate waiting during the delay period\n",
    "    return 0.0\n",
    "\n",
    "# This is the function that will be used to evaluate whether the PFC outputs the correct rule for a given cue, and to train it to do so\n",
    "def cue_to_rule_mapping_function(t, integer_input): # Define a changing mapping from cues to rules\n",
    "    \n",
    "    if t >= end_time_of_delay: # We can use this type of conditioning to set up time-dependent cue-to-rule mappings\n",
    "        \n",
    "        r1 = tf.cond(tf.equal(integer_input, tf.constant(0, dtype = tf.float32)), true_fn = rvis, false_fn = rnull)\n",
    "    \n",
    "        r2 = tf.cond(tf.equal(integer_input, tf.constant(1, dtype = tf.float32)), true_fn = rvis, false_fn = rnull)\n",
    "        \n",
    "        r3 = tf.cond(tf.equal(integer_input, tf.constant(2, dtype = tf.float32)), true_fn = raud, false_fn = rnull)\n",
    "        \n",
    "        r4 = tf.cond(tf.equal(integer_input, tf.constant(3, dtype = tf.float32)), true_fn = raud, false_fn = rnull) \n",
    "    \n",
    "        return tf.reduce_max([r1,r2,r3,r4])\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return tf.constant(rnull(), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for evaluating correctness\n",
    "def correctness(q, o, ina, inv, ctin, s): # Arguments: time, PFC output list, auditory input list, vision input list, cue inputs timeline. session\n",
    "    \n",
    "    if q >= end_time_of_delay: # In this case, should be executing a rule\n",
    "    \n",
    "        c = []\n",
    "        for e in range(batch_size):\n",
    "            if (np.linalg.norm(inv[e,q] - o[q][e]) < np.linalg.norm(ina[e,q] - o[q][e]) and s.run(cue_to_rule_mapping_function(q, float(ctin[e,start_time_of_delay-1]))) == 1.0):\n",
    "                c.append(\"correct\") # If the output coordinates are closer to the vision target than the auditory target and that's the right rule\n",
    "            elif (np.linalg.norm(inv[e,q] - o[q][e]) > np.linalg.norm(ina[e,q] - o[q][e]) and s.run(cue_to_rule_mapping_function(q, float(ctin[e,start_time_of_delay-1]))) == 2.0):\n",
    "                c.append(\"correct\") # If the output coordinates are closer to the auditory target than the vision target and that's the right rule\n",
    "            else:\n",
    "                c.append(\"incorrect\")\n",
    "        return c\n",
    "    \n",
    "    else: # In this case, should output something close to the origin coordinates (0,0)\n",
    "        c = []\n",
    "        for e in range(batch_size):\n",
    "            if np.linalg.norm([0.0,0.0] - o[q][e]) < 0.01:\n",
    "                c.append(\"correct\")\n",
    "            else:\n",
    "                c.append(\"incorrect\")\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the network\n",
    "\n",
    "DO_SHARE = None # This is needed for the recurrency of the RNN, see: https://blog.evjang.com/2016/06/understanding-and-implementing.html\n",
    "\n",
    "PFC_state_previous = tf.contrib.rnn.LSTMBlockCell(num_units = num_units_PFC).zero_state(batch_size, tf.float32) # Initial state of PFC\n",
    "\n",
    "# This does one cycle of the PFC RNN\n",
    "def PFC_step(input_data, network_state):\n",
    "    with tf.variable_scope(\"PFC\", reuse=DO_SHARE):\n",
    "        PFC_cell = tf.contrib.rnn.LSTMBlockCell(num_units = num_units_PFC)\n",
    "        return PFC_cell(inputs = input_data, state = network_state) \n",
    "    \n",
    "# Cue inputs delivered INTO the PFC, e.g., high pass noise, UV light, and so forth, here defined currently as just an integer indiciating which cue\n",
    "cue_timeseries = tf.placeholder(shape=[batch_size, num_timesteps, 1], dtype=tf.float32, name = 'cues_timeseries')\n",
    "\n",
    "# Visual and audio targets input to the sensory-motor system, represented as just 2D coordinates, one for each, right now\n",
    "input_vis = tf.placeholder(shape=[batch_size, num_timesteps, 2], dtype=tf.float32, name = 'input_vis')\n",
    "input_aud = tf.placeholder(shape=[batch_size, num_timesteps, 2], dtype=tf.float32, name = 'input_aud')\n",
    "\n",
    "loss = 0 # This will accumulate contributions to the loss function\n",
    "outs = [] # Output coordinates from the sensory-motor system\n",
    "rules_chosen = [] # Rules input from the PFC to the sensory-motor system\n",
    "\n",
    "pre_delay_cue = cue_timeseries[:,start_time_of_delay - 1]\n",
    "\n",
    "for t in range(num_timesteps): # Iterating over time in the experiment, with batches implicitly being used at each time step\n",
    "    \n",
    "    current_cue = cue_timeseries[:,t] # This is the cue that gets fed into the PFC on this timestep\n",
    "\n",
    "    PFC_state = PFC_step(input_data = current_cue, network_state = PFC_state_previous)\n",
    "    # The output from the PFC into the sensorymotor system gives information about the rule that the sensory-motor system is supposed to execute\n",
    "    with tf.variable_scope(\"PFC_output\", reuse=DO_SHARE):\n",
    "        PFC_output = tf.nn.dropout(tf.contrib.layers.fully_connected(PFC_state[0], PFC_output_dimensions, activation_fn = None), keep_prob = 0.5) # Note the 1-dimensional rule output; also this layer is linear\n",
    "    rules_chosen.append(PFC_output) # The info sent by PFC to the sensory-motor system, corresponding to the chosen rule\n",
    "\n",
    "    input_rule_to_sensorymotor_system = PFC_output # The rule output from the PFC is sent to the sensory-motor system\n",
    "    input_total = tf.concat([input_vis[:,t], input_aud[:,t], input_rule_to_sensorymotor_system], axis = -1) # Sensory-motor system gets aud, vis & rule inputs\n",
    "    \n",
    "    with tf.variable_scope(\"hidden_sensorymotor\", reuse=DO_SHARE):\n",
    "        hidden_sensorymotor = tf.nn.dropout(tf.contrib.layers.fully_connected(input_total, num_hidden_sensorymotor, activation_fn = tf.nn.relu), keep_prob = 0.5) # Sensory-motor hidden layer\n",
    "    \n",
    "    num_out_sensorymotor = 2 # These are the 2D coordinates that we want to sensory-motor system to produce\n",
    "    \n",
    "    with tf.variable_scope(\"out_sensorymotor\", reuse=DO_SHARE):\n",
    "        out_sensorymotor = tf.contrib.layers.fully_connected(hidden_sensorymotor, num_out_sensorymotor, activation_fn = tf.nn.relu) \n",
    "        # Output coord from sensory-motor system\n",
    "    \n",
    "    outs.append(out_sensorymotor)\n",
    "    \n",
    "    PFC_state_previous = PFC_state[1] # This is needed for the recurrency of the PFC RNN\n",
    "    \n",
    "    for e in range(batch_size): # Here is where we compute a contribution to the loss function for training\n",
    "        cpred = pre_delay_cue[e][0] # The cue we are supposed to be holding on to in working memory\n",
    "        cue_to_rule_mapping_function_output = cue_to_rule_mapping_function(t, cpred) # The CORRECT rule to choose in the task\n",
    "        if t >= end_time_of_delay: # We use this to set up time-dependent loss functions, e.g., only look at late times\n",
    "            loss_contrib1 = tf.cond(tf.equal(cue_to_rule_mapping_function_output, tf.constant(1, dtype = tf.float32)),\\\n",
    "                                    true_fn = lambda: tf.square(tf.norm(input_vis[e,t] - out_sensorymotor[e])),  false_fn = lambda: 0.0)\n",
    "            loss_contrib2 = tf.cond(tf.equal(cue_to_rule_mapping_function_output, tf.constant(2, dtype = tf.float32)),\\\n",
    "                                    true_fn = lambda: tf.square(tf.norm(input_aud[e,t] - out_sensorymotor[e])),  false_fn = lambda: 0.0)\n",
    "            # The last two lines define the loss function contribution at a given time, \n",
    "            # here the proximity of the output coordinate to the visual input target OR the proximity to the auditory target,\n",
    "            # depending on the correct rule for this moment in the task\n",
    "            loss += (loss_contrib1 + loss_contrib2)\n",
    "        else:\n",
    "            loss += resting_prefactor * tf.reduce_sum(tf.square(out_sensorymotor[e])) # Distance of the output from the origin...\n",
    "            \n",
    "    DO_SHARE = True # This is needed for the recurrency of the RNN, see: https://blog.evjang.com/2016/06/understanding-and-implementing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trained variables here are:\n",
      "\tPFC_output/fully_connected/weights:0\n",
      "\tPFC_output/fully_connected/biases:0\n",
      "\thidden_sensorymotor/fully_connected/weights:0\n",
      "\thidden_sensorymotor/fully_connected/biases:0\n",
      "\tout_sensorymotor/fully_connected/weights:0\n",
      "\tout_sensorymotor/fully_connected/biases:0\n"
     ]
    }
   ],
   "source": [
    "# Setting up the session: standard TensorFlow boilerplate\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# We are going to specify the trainable variables here, leaving out the PFC internal connections\n",
    "trained_vars_list = []\n",
    "for name in [\"PFC_output\", \"hidden_sensorymotor\", \"out_sensorymotor\"]:\n",
    "    trained_vars_list.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, name))\n",
    "if PFC_trainable:\n",
    "    trained_vars_list.extend(tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"PFC\"))\n",
    "    \n",
    "print \"The trained variables here are:\"\n",
    "for v in trained_vars_list:\n",
    "    print \"\\t\" + v.name\n",
    "    \n",
    "train_op=optimizer.minimize(loss, var_list = trained_vars_list)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the cue timeseries input provided by the environment\n",
    "# Here random cues are delivered across time and within batch\n",
    "ct = np.reshape([np.random.randint(0,4) for k in range(num_timesteps * batch_size * num_batches)], [num_batches, batch_size, num_timesteps, 1])\n",
    "    \n",
    "# Randomly chosen visual target coordinates\n",
    "in_vis_list_unshaped = [[np.random.rand(), np.random.rand()] for k in range(num_timesteps * batch_size * num_batches)] \n",
    "in_vis_list = np.reshape(in_vis_list_unshaped, [num_batches,batch_size,num_timesteps, 2])\n",
    "\n",
    "# Randomly chosen auditory target coordinates\n",
    "in_aud_list_unshaped = [[np.random.rand(), np.random.rand()] for k in range(num_timesteps * batch_size * num_batches)]\n",
    "in_aud_list = np.reshape(in_aud_list_unshaped, [num_batches,batch_size,num_timesteps, 2])\n",
    "\n",
    "# Running the session and storing the loss function across batches to see the learning curve  \n",
    "losses = []\n",
    "for b in range(num_batches): # Running the training batches\n",
    "    ct_in = ct[b, :, :]\n",
    "    in_v = in_vis_list[b, :, :]\n",
    "    in_a = in_aud_list[b, :, :]\n",
    "    os, l, rc, _ = sess.run([outs, loss, rules_chosen, train_op], feed_dict = {cue_timeseries:ct_in, input_vis: in_v, input_aud:in_a})\n",
    "    # The outputs here are just for the last batch only...\n",
    "    losses.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Printing the outputs for visual inspection\n",
    "print \"Loss: %f\" % l\n",
    "print \"\\n\\n\"\n",
    "\n",
    "rule_to_hold_onto = [sess.run(cue_to_rule_mapping_function(end_time_of_delay, float(ct_in[e,start_time_of_delay-1]))) for e in range(batch_size)]\n",
    "\n",
    "for q in range(num_timesteps):\n",
    "    print \"timestep %i\" % q\n",
    "    print \"out\", os[q][:], \"\\n\", \"vis in\", in_v[:,q],\"\\n\", \"aud in\", in_a[:, q],\"\\n\", \"PFC output\", rc[q][:] \n",
    "    print \"rule given cue just prior to delay period:\", rule_to_hold_onto\n",
    "    print \"correctness:\", correctness(q, os, in_a, in_v, ct_in, sess) \n",
    "    print \"\\n\"\n",
    "    \n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth\n",
    "\n",
    "# Plotting the loss function over time to see the learning process\n",
    "plt.figure()\n",
    "plt.plot(smooth(losses, 10))\n",
    "plt.title(\"Loss versus # batches\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
